<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>推荐系统之embedding - Knowledge Change Destiny</title>
        <meta name="keywords" content="Mechine Learning , Economics , Mathematics"/>
        <meta name="description" content="More Reading , More Possibilities"/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)'], ['$', '$']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Google Adsense -->
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');


            // Google Adsense Auto AD
            (adsbygoogle = window.adsbygoogle || []).push({});
            /*
             (adsbygoogle = window.adsbygoogle || []).push({
                  google_ad_client: "ca-pub-6300557868920774",
                  enable_page_level_ads: true
             });
             */
        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#推荐系统">推荐系统</a>&nbsp;»&nbsp;推荐系统之embedding</div>
</div>
<div class="clearfix"></div>
<div id="title">推荐系统之embedding</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#2014-item2vec-neural-item-embedding-for-collaborative-filtering">[2014微软-Item2Vec]: Neural Item Embedding for Collaborative Filtering</a></li>
<li><a href="#2018billion-scale-commodity-embedding-for-e-commerce-recommendation-in-alibaba">[2018阿里]：Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</a></li>
<li><a href="#2014deepwalk-online-learning-of-social-representations">[2014]DeepWalk: Online Learning of Social Representations</a></li>
<li><a href="#2015msraline-large-scale-information-network-embedding">[2015MSRA]LINE: Large-scale Information Network Embedding</a></li>
<li><a href="#2001laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering">[2001谱聚类]Laplacian eigenmaps and spectral techniques for embedding and clustering</a></li>
</ul>
</div>
<p>Embedding可以看成是一种降维技术。和传统的降维技术相比有啥区别呢？=&gt; 能够发挥大语料的作用。</p>
<p>怎么使用SVD来做序列的Embedding？=&gt;对于一个Item，计算在所有序列上窗口w内，出行其他的item的计数，构成矩阵。</p>
<p>Embedding，通过共现性，学习item潜在隐义。NLP中：语义、句法等。Airbnb：风格、主题。</p>
<p>Graph Embedding有3个方向：</p>
<ol>
<li>
<p>Factorization methods。</p>
<p>Distributed large-scale natural graph factorization.2013 ,WWW.</p>
<p>MDS、IsoMap、Laplacina eigenmap.</p>
</li>
<li>
<p>Deep learning methods。</p>
<p>Deep neural networks for learning graph representations . 2016, AAAI.</p>
<p>Structural deep network embedding . 2016,KDD.</p>
<p>Transnet: Translation-based network representation learning for social relation extraction. 2017,IJCAI</p>
</li>
<li>
<p>Random Walk based techniques。</p>
</li>
</ol>
<p>metapath2vec: Scalable representation learning for heterogeneous networks.2017,KDD</p>
<p>Node2vec: Scalable feature learning for networks .2016,KDD</p>
<p>Deepwalk: Online learning of social representations.2014,KDD</p>
<h4 id="2014-item2vec-neural-item-embedding-for-collaborative-filtering">[2014微软-Item2Vec]: Neural Item Embedding for Collaborative Filtering</h4>
<p>提出了Item2vec，并和SVD做了实验对比。</p>
<ol>
<li>Item2vec的算法原理是什么？如何构造数据？如何训练及参数更新？最后得到什么？怎么做协同过滤？</li>
</ol>
<p>​        item set 内的item对构成正样本。使用负采样的方式训练，得到向量。</p>
<ol>
<li>如何做的对比？</li>
</ol>
<p>​    做歌手音乐流派测试，随机选择top k个歌手，看ta是否和他的近邻的标签是否一致，一致则分类准确，基于Item2vec的方法准确率更高。论文中提高，这个方法可以用来做标签校正。</p>
<ol>
<li>解决了什么问题？优势是什么？</li>
</ol>
<p>​      传统Item-based CF基于用户-item-用户的隐式链接，需要user信息，而Item2Vec CF 使用item间的关系，可以不需要用户信息，直接学习item间关系。更利于 exploration、discovery，增加些非同质化的内容。</p>
<ol>
<li>对不同频次的item,在采样概率上有处理。</li>
</ol>
<h4 id="2018billion-scale-commodity-embedding-for-e-commerce-recommendation-in-alibaba">[2018阿里]：Billion-scale Commodity Embedding for E-commerce Recommendation in Alibaba</h4>
<p>有向图。</p>
<p>使用用户的历史行为构建Item Graph，然后使用算法学得Graph中每个Item的Embedding向量。</p>
<p><code>论文要解决的问题是</code>：<code>引入Side information，改进Graph Embedding框架，缓解数据稀疏及冷启动问题</code>。</p>
<p><code>Side information</code>: category、brand、price....。比如：新上一款索尼 微单，有了品牌和类别信息，可以便捷的推荐给浏览过 相关品牌和类别的用户，如果只使用item信息，没有浏览，将没有Graph边。稀疏数据的问题，使用了品牌信息，粒度更大，数据将不会过于稀疏。</p>
<p><strong>为什么使用向量召回</strong>？</p>
<p>和传统的CF方法相比：原有方法只考虑了item共现性（co-occurrence)，忽略了时序、序列上的信息。对于item序列的使用，不能使用用户历史所有的记录，一是因为数据量过大，二是兴趣潜在变化。因此一般使用 session内的item 序列。阿里经验上使用的时间窗是<code>1H</code>。</p>
<p><strong>图构建和Embedding求解</strong>：如果session内，两个item接连出现，则第一个item存在一条指向第二个item的有向边。<strong>权重等于频次</strong>。序列列构建，采用random walk，节点转移概率为归一化的频次。由item Embedding和side information Embedding加权得到最终的Embedding向量。一种做法是等权重加权，另一种做法是算法自动学习权重。</p>
<p><strong>数据清洗</strong>：1. 停留时间小于1秒的item。2. 过于活跃的用户，直接去除。3. 商品可能存在更新，用户点击了一次之后，再次点击的时候，可能发生了改变，这种情况也去除用户的数据。</p>
<p><img src="/wiki/static/images/GraphEmbedding1.png"  alt="GE"/></p>
<p><strong>结果验证</strong>：亚马逊数据集，验证下AUC。线上AB Test。可视化足球、乒乓球、羽毛球类的item，PCA降维后看分布。发现item所属的商家对最后Embedding贡献最大。</p>
<p><strong>item冷启动</strong>：使用side information Embedding 的均值作为新商品的Embedding向量。</p>
<h4 id="2014deepwalk-online-learning-of-social-representations">[2014]DeepWalk: Online Learning of Social Representations</h4>
<p>无向图</p>
<ol>
<li>针对什么场景？研究了什么问题？用了什么方法？得到什么结论？ </li>
</ol>
<p>针对网络节点的表征学习，提出了DeepWalk方法。采用连续向量对节点进行编码表示。语言模型和非监督学习方法的推广使用。对于多标签的分类任务，基于本文提出的DeepWalk方法，F1得分比其他方法搞了10%。</p>
<ol>
<li>
<p>关于长尾分布（power law distribution)的处理？</p>
</li>
<li>
<p>序列如何生成？权重怎么构建？转移概率？</p>
</li>
</ol>
<p>​       对每一个节点，都采样$\lambda$次，转移概率使用均匀分布，序列长度可设置。</p>
<ol>
<li>
<p>论文实验结果</p>
<p>先吹一波比其他方法好，主要使用F1得分。关于参数上的实验 skip-gram 窗口大小、$\lambda$大小、采样的序列长度。</p>
</li>
</ol>
<p>和其他方法相比：只使用局部信息；在图领域引入无监督的表示学习。</p>
<p><img src="/wiki/static/images/GraphEmbedding1.png"  alt="DeepWalk"/></p>
<p>考虑四个要素：1. Adaptability。适应性。对新样本不需要全部重新训练。2.Community aware。距离计算，相当于实际网络中的用户相似性。3.Low dimensional。低纬度数据<strong>泛化能力更强</strong>，收敛和推断更快。4. Continuous。连续空间的表达，提供更平滑的分类边界。社交网络中需要对用户进行分类。</p>
<h4 id="2015msraline-large-scale-information-network-embedding">[2015MSRA]LINE: Large-scale Information Network Embedding</h4>
<p><a href="https://github.com/tangjianpku/LINE">LINE源码</a></p>
<ol>
<li>针对什么场景？研究了什么问题？用了什么方法？得到什么结论？ </li>
</ol>
<p>针对大规模网络，提出 LINE算法解决节点低维表征的方法。适用于有向、无向、带权等图网络；目标函数能保留局部和全局的结构信息；Edge-Sampling算法，改善学习和推断的效率。</p>
<p>经典方法MDS、IsoMap、Laplacian eigenmap等，计算时间复杂度过高。</p>
<ol>
<li>first-order proximity </li>
</ol>
<h4 id="2001laplacian-eigenmaps-and-spectral-techniques-for-embedding-and-clustering">[2001谱聚类]Laplacian eigenmaps and spectral techniques for embedding and clustering</h4>
</div>
<div id="income">
    <img src="/wiki/static/images/galaxy.jpg" alt="星空" style="max-width:300px;" />

    <ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
</div>
<div id="content-footer">created in <span class="create-date date"> 2019-05-18 17:20 </span></div>

<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: '推荐系统之embedding',
  owner: 'RbingChen',
  repo: 'wiki',
  oauth: {
    client_id: '8d8c2034c8f3db5fd412',
    client_secret: 'b5dabd27e8c79eb5136fba7730d78f403ea54991',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2019 Cimon.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>