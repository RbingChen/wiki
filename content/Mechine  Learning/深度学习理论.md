---
title : "Deep learning theory"
layout : page
date : 2019-02-24 14:01
---

[TOC]



# 一 deep vs  shallow

## 1.1定义

Universality定理: Shallow network 可以fit 任何函数。

满足条件
$$
||f(x_1)-f(x_2)||\le L||x_1-x_2||
$$
的函数，被称为L-Lipschitz Function，Lipschitz条件用以保证函数足够smooth，以及函数的连续性，不能确保函数处处可微。

## 1.2证明

简化处理，在$[0,1]$区间内考虑函数的拟合，给定误差$\epsilon$的情况下，如果函数满足
$$
\int_{0}^{1}|f(x)-f^{\ast}(x)|^2 dx\le\epsilon
$$
则认为存在一个函数能够拟合函数$f(x)$。给出一更严格的定义：
$$
\mathop{\max}_{0\le x\le1}|f(x)-f^\ast(x)|\ \ \le \epsilon
$$
<img src="/wiki/static/images/DeepThoery1.png"  alt="曲线拟合"/>

使用线性的片段函数(piecewise linear function)对函数进行拟合。

<img src="/wiki/static/images/DeepThoery2.png"  alt="切片"/>

使用relu函数进行叠加，能够拟合出不同的片段。只要使用足够的神经元，Shallow network可以拟合任何函数。

<img src="/wiki/static/images/DeepThoery3.png"  alt="推论"/>

<img src="/wiki/static/images/DeepThoery4.png"  alt="推论"/>

由[1,2]的推论可知，相同神经元的情况下，deep network将有更多的linear region，且随深度指数增加。

论文[3]，从实验的角度证明了，linear region的数量随着深度而指数增加。

<img src="/wiki/static/images/DeepThoery5.png"  alt="推论"/>

如上图，左图表示神经元固定时，深度越深，transition(论文的transition描述的是，转折点，如relu的0点，与linear region等效)指数增加。此外，论文从实验角度证明了，网络的不同层对最终结果的影响程度不一样，越是low的层，对结果影响越大。很符合直观上地认识，浅层提取的信息保真度越高，才能保证deep层的有效处理。

架构不同，一般用同样的参数量进行比较不同网络的好坏。

Q1:需要多少神经元去拟合一个 L-Lipschitz function f。

参数更多，表征了更强。
$$
w_1x_1+...+w_nx_n
$$

$$
w_1x_1x_2+w_2x_3x_4
$$

深度网络，出现更多的交叉或者是高阶项。低阶项难以表述高阶。从这里可以思考，deepctr模型，dnn部分用来学习更高阶(更高层交叉，隐式交叉)的特征。

空间上的拟合，是不是有点像是决策树了，决策树是已知数值上进行求解，不能得到未知的数值(值域确定)，而NN不一样，可以通过特征组合，得到不同的值(值域不确定)。



## 1.3 过拟合

Deep 的拟合能力强，是否存在过拟合问题？

直接使用全连接不就行了，为啥还需要CNN、lstm。

lstm：解决时隙问题，cnn可以取代了。



为什么deep 会比 shallow好？

1. 逻辑电路，更加有效。2. 能表达更复杂的关系。3.从参数量来说，其实同样的神经元deep模型的参数量会更多，也就意味着能够拟合更复杂的函数。

参数量的区别？

为什么不考虑sigmoid？很难进行理论分析。相当于是很多sigmoid 函数值的叠加。

另一方面，考虑relu的取值范围是[0,1]，是否表征能力会跟强。全连接导致梯度爆炸，所以lstm一般不用relu？

# 参考论文

1. Razvan Pasacanu , On the number of response regions of deep feedforward networks with piecewise linear activations,ICLR,2014
2. G.F Montufar , On the Number of Linear Regions of Deep Neural Networks,NIPS,2014
3. M. Rahu , On the Expressive Power of Deep Neural Networks ,ICML,2017