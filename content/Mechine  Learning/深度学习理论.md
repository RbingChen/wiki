---
title : "Deep learning theory"
layout : page
date : 2019-02-24 14:01
---

[TOC]



# 一 deep vs  shallow

## 1.1定义

Universality定理: Shallow network 可以fit 任何函数。

满足条件
$$
||f(x_1)-f(x_2)||\le L||x_1-x_2||
$$
的函数，被称为L-Lipschitz Function，Lipschitz条件用以保证函数足够smooth，以及函数的连续性，不能确保函数处处可微。

## 1.2证明

简化处理，在$[0,1]$区间内考虑函数的拟合，给定误差$\epsilon$的情况下，如果函数满足
$$
\int_{0}^{1}|f(x)-f^{\ast}(x)|^2 dx\le\epsilon
$$
则认为存在一个函数能够拟合函数$f(x)$。给出一更严格的定义：
$$
\mathop{\max}_{0\le x\le1}|f(x)-f^\ast(x)|\ \ \le \epsilon
$$
<img src="/wiki/static/images/DeepThoery1.png"  alt="曲线拟合"/>

使用线性的片段函数(piecewise linear function)对函数进行拟合。

<img src="/wiki/static/images/DeepThoery2.png"  alt="切片"/>

使用relu函数进行叠加，能够拟合出不同的片段。





架构不同，一般用同样的参数量进行比较不同网络的好坏。

Q1:需要多少神经元去拟合一个 L-Lipschitz function f。

参数更多，表征了更强。
$$
w_1x_1+...+w_nx_n
$$

$$
w_1x_1x_2+w_2x_3x_4
$$

深度网络，出现更多的交叉或者是高阶项。低阶项难以表述高阶。从这里可以思考，deepctr模型，dnn部分用来学习更高阶(更高层交叉，隐式交叉)的特征。

空间上的拟合，是不是有点像是决策树了。



为什么deep 会比 shallow好？

1. 逻辑电路，更加有效。2. 能表达更复杂的关系。3.从参数量来说，其实同样的神经元deep模型的参数量会更多，也就意味着能够拟合更复杂的函数。

参数量的区别？

为什么不考虑sigmoid？很难进行理论分析。相当于是很多sigmoid 函数值的叠加。

另一方面，考虑relu的取值范围是[0,1]，是否表征能力会跟强。全连接导致梯度爆炸，所以lstm一般不用relu？

# 参考论文

1. Razvan Pasacanu , On the number of response regions of deep feedforward networks with piecewise linear activations,ICLR,2014
2. G.F Montufar , On the Number of Linear Regions of Deep Neural Networks,NIPS,2014
3. M. Rahu , On the Expressive Power of Deep Neural Networks ,ICML,2017