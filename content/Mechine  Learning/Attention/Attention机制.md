---
title : "attention"
layout : page
date : 2018-11-01 16:04
---

[TOC]



# 一  attention 原理





# 二 attention分类





## 2.1基础的attention结构

1. hard attention 、soft attention

​    *Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*



2. global attention 、local attention

​     *Effective Approaches to Attention-based Neural Machine Translation*

  



3. 

​           *neural machine translation by jointly learning to align and translate*

 4.self-attention

​         *attention is all you need*

​         *weighted transformer network for machine translation*



## 2.2 组合的attention结构





# 三 应用





参考：

https://blog.csdn.net/xiewenbo/article/details/79382785

