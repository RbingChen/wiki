---
title : "attention"
layout : page
date : 2018-11-01 16:04
---

[TOC]

首先给一定典型的attention结构图。

<img src="/wiki/static/images/attention.png" alt="attention图" />

有如下定义：

<img src="/wiki/static/images/attention_equation_0.jpg" alt="attention计算" />

$h_t$是decode端 hidden state , $\overline{h}_s$是编码端hidden state

socre 有两种计算方法：

<img src="/wiki/static/images/attention_equation_1.jpg" alt="attention计算" />



# 一  attention 原理





# 二 attention分类





## 2.1基础的attention结构

1. hard attention 、soft attention

​    *Show, Attend and Tell: Neural Image Caption Generation with Visual Attention*



2. global attention 、local attention

​     *Effective Approaches to Attention-based Neural Machine Translation*

  



3.  *neural machine translation by jointly learning to align and translate*
4. self-attention

​         *attention is all you need*

​         *weighted transformer network for machine translation*



## 2.2 组合的attention结构





# 三 应用





参考：

https://blog.csdn.net/xiewenbo/article/details/79382785

