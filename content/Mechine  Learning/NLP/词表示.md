---
title : "word represent"
layout : page
date : 2019-03-18 17:20
---

[TOC]

# 零.前置问题

## 0.1什么是语言模型？

给定词$w_1,w_2,...,w_n$，构造成句子$P(w_1,w_2,...,w_n)$的概率就是语言模型。即：用来衡量任意短文本出现概率的模型，叫做`语言模型`。主要有统计语言模型(n-gram)和神经网络语言模型(NNLM,RNNLM)两种。



## 0.2什么是词向量？

使用固定长度的向量来表示词典中任意词，该向量称为词向量。又叫word embedding，word representation。

## 0.3 词表示的假设

VSM到word2vec到Glove。

`Distributional semantics: A word’s meaning is given by the words that frequently appear close-by `

**词及其相近词的共现频次决定了词的意义；词$w$的不同上下文决定了$w$的表示。**





# 一.Vector Space Model

1. 新词需要重新训练，word2vec 只需要训练出现的新词的句子。
2. 很多词和词共现频次低。因此对于高频共现词和低频，需要做处理。
3. 矩阵构建较难，难以训练。。
4. 能很好的构建文档的向量表达，充分利用统计信息，全局优化。



# 二.word2vec

## 核心思想

大语料；每一个词在词典中有唯一对应的向量；每个词都有上下文词；使用词及其上下文词的词向量的相似度进行概率计算；调整词向量，使得概率最大。

skip-gram：
$$
\mathop{\arg\max}_\limits{V} \sum_j \sum_{n=-k}^{k}logP(c_{j+n}|w_j)
$$
其中$P(c_{j+n}|w_j)=\frac{exp(v_w^Tv_c)}{\sum_{o\in V} exp(v_w^Tv_o)}$，$v$代表词向量。

## 结构

Skip-grams（预测词围绕中心词），Continuous Bag of Words(CBow，预测词在中间)。

<img src="/wiki/static/images/word2vec1.png" alt="word2vec" />

## 优化

1. Negative Sampling
2. hierarchical softmax 

# 三 Glove

# 参考文献