---
title : "word represent"
layout : page
date : 2019-03-18 17:20
---

[TOC]

# 零.前置问题

## 0.1什么是语言模型？

给定词$w_1,w_2,...,w_n$，构造成句子$P(w_1,w_2,...,w_n)$的概率就是语言模型。即：用来衡量任意短文本出现概率的模型，叫做`语言模型`。主要有统计语言模型(n-gram)和神经网络语言模型(NNLM,RNNLM)两种。



## 0.2什么是词向量？

使用固定长度的向量来表示词典中任意词，该向量称为词向量。又叫word embedding，word representation。

一词多义问题；引入先验知识；词向量可解释性。

## 0.3 词表示的假设

VSM到word2vec到Glove。

`Distributional semantics: A word’s meaning is given by the words that frequently appear close-by `

**词及其相近词的共现频次决定了词的意义；词$w$的不同上下文决定了$w$的表示。**





# 一.Vector Space Model

1. 新词需要重新训练，word2vec 只需要训练出现的新词的句子。
2. 很多词和词共现频次低。因此对于高频共现词和低频，需要做处理。
3. 矩阵构建较难，难以训练。。
4. 能很好的构建文档的向量表达，充分利用统计信息，全局优化。



# 二.word2vec

## 核心思想

大语料；每一个词在词典中有唯一对应的向量；每个词都有上下文词；使用词及其上下文词的词向量的相似度进行概率计算；调整词向量，使得概率最大。

skip-gram：$\mathop{\arg\max}_\limits{V} \sum_j \sum_{n=-k}^{k}logP(c_{j+n}|w_j)$

其中$P(c_{j+n}|w_j)=\frac{exp(v_w^Tu_c)}{\sum_{o\in V} exp(v_w^Tu_o)}$，$v$代表词向量。

softmax 使用的$U$向量（上下文矩阵）和词向量（词向量矩阵）是不同，这样使得问题更容易优化（求解更方便？），也可以使用同一个向量。



## 结构

Skip-grams（预测词围绕中心词），Continuous Bag of Words(CBow，预测词在中间)[2]。

<img src="/wiki/static/images/word2vec1.png" alt="word2vec" />

为什么会有Skip-gram和CBow，而不是采用n-gram?

​      n-gram只使用前向词，而不使用后向词。词和其前后的词的相关性更大，只用前或者后的词有所欠缺。

Skip-grams 与CBow对比？

## 优化

### Negative Sampling

 思想是什么？

给定中心词$w$和上下文词$c$，随机选择的其他词都是负样本。每个词出现的频率不同，需要做处理；随机的词不能是$c$。

### hierarchical softmax 

 构建一个树来做决策，每个节点有一个向量变量用来判断是往左还是右子树走。

词向量和每个节点向量分别做内积，计算sigmoid值。可以发现，HS方法的下上文矩阵不是对应于具体的词。

## 优缺点



1.能很好地应用到其他的nlp 任务，且效果不错。

2.不仅仅捕获词相似度信息，也能学习到更复杂的模式。而VSM主要是用以捕获词的相似度。

3.但没有充分利用统计信息。VSM模型利用了，这也导致VSM由于词频问带来的词之间重要性的不均衡问题。



`word2vec 为什么有效`？

 

# 三 Glove



$g_i$表示损失函数的一阶导数，$h_i$表示损失函数的二阶导数。$G_L、H_L、G_R、H_R$表示中间计算量。

# 参考文献

1. T.Mikolov,Distributed Representations of Words and Phrases and their Compositionality。负采样paper
2. T.Mikolov,Exploiting Similarities among Languages for Machine Translation,2013。