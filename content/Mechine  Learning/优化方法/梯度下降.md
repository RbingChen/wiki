---
title : "梯度下降"
layout : page
date : 2018-12-18 23:41
---

[TOC]



# 一 Gradient Descent 为什么能求最小值？

## 1.普通的梯度下降

首先考虑泰勒展开：
$$
f(x)=f(x_0)+f^{'}(x_0)(x-x_0)+\frac{f^{''}(x_0)}{2}(x-x_0)^2+......
$$
当$x,x_0$ 很接近的时候，近似等于：
$$
f(x)\approx f(x_0)+f^{'}(x_0)(x-x_0)
$$
这是一个一元二次函数，要函数减小，则可以有：
$$
1. f^{'}(x_0)>0\ \ \ \ =>\ \ \ \ x<x_0\\\\
2.f^{'}(x_0)<0\ \ \ \ =>\ \ \ \ x>x_0
$$
可以表示为：
$$
x-x_0=-\alpha*f^{'}(x_0)
$$

## 2.二阶的梯度下降

牛顿法，直接考虑二阶，这个时候是一个二次函数，二次函数的最小值：
$$
x=x_0-\frac{f^{'}(x_0)}{f^{''}(x_0)}
$$
对于多元函数，$f^{''}(x_0)$ 是一个矩阵，Hessian矩阵$H$。
$$
x=x_0-H^{-1}f^{'}(x_0)
$$
当变量较多的时候，$H^{-1}$ 的求解，很复杂。因此，延伸出拟牛顿法，目的是用近似的方法来计算Hessian矩阵的逆。

关于学习率的解释：泰勒展开是在$x_0$的很小的邻域内，如果$x$超过这个邻域，会导致推导不成立，函数值不会减小。

# 二 SGD 和 GD



stochastic gradient descent ：考虑单个样本

gradient descent：考虑整体样本。

学习率的设置很关键



# 三 梯度优化方法的改进