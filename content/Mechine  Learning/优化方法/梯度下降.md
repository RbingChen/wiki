---
title : "梯度下降"
layout : page
date : 2018-12-18 23:41
---

[TOC]



# 一 Gradient Descent 为什么能求最小值？

## 1.普通的梯度下降

首先考虑泰勒展开：
$$
f(x)=f(x_0)+f^{'}(x_0)(x-x_0)+\frac{f^{''}(x_0)}{2}(x-x_0)^2+......
$$
当$x,x_0$ 很接近的时候，近似等于：
$$
f(x)\approx f(x_0)+f^{'}(x_0)(x-x_0)
$$
这是一个一元二次函数，要函数减小，则可以有：
$$
1. f^{'}(x_0)>0\ \ \ \ =>\ \ \ \ x<x_0\\\\
2.f^{'}(x_0)<0\ \ \ \ =>\ \ \ \ x>x_0
$$
可以表示为：
$$
x-x_0=-\alpha*f^{'}(x_0)
$$

## 2.二阶的梯度下降

牛顿法，直接考虑二阶，这个时候是一个二次函数，二次函数的最小值：
$$
x=x_0-\frac{f^{'}(x_0)}{f^{''}(x_0)}
$$
对于多元函数，$f^{''}(x_0)$ 是一个矩阵，Hessian矩阵$H$。
$$
x=x_0-H^{-1}f^{'}(x_0)
$$
当变量较多的时候，$H^{-1}$ 的求解，很复杂。因此，延伸出拟牛顿法，目的是用近似的方法来计算Hessian矩阵的逆。

关于学习率的解释：泰勒展开是在$x_0$的很小的邻域内，如果$x$超过这个邻域，会导致推导不成立，函数值不会减小。

# 二 SGD 和 BGD

## batch gradient descent

考虑整体样本。求解慢且计算所有样本耗内存多；因为要使用到所有样本导致无法在线更新模型。能够确保收敛一个全局最优或者局部最优。

## stochastic gradient descent 

考虑单个样本。更新快，可在线跟新；目标函数值波动大，使得函数可能跳过潜在的局部最小值，但也会可能跳过全局最优值，导致最终很难收敛。通过降低学习率，sgd能和bgd一样收敛。

## Mini-batch gradient descent

考虑部分样本。综合了sgd和bgd，折中。降低参数波动，提高更新速度。

**学习率的设置很关键**

几个问题：

1. 合适的学习率的选择较难。
2. 训练过程中的学习率更新策略。
3. 所有参数是否使用同一个学习率。稀疏数据中，部分特征数据较少。
4. 避免陷入局部最优。DNN等。



# 三 梯度优化方法的改进

## Momentum

  减小震荡，避免更新方向和原来的方向相差过大。
$$
v_t=\gamma v_{t-1}+\eta \triangledown _\theta J(\theta)\\\\
\theta =\theta -v_t
$$
如同，从把一个球推到山低，球在滚落的过程中动量在增加，越来越快。在更新模型参数时，对于那些当前的梯度方向与上一次梯度方向相同的参数，那么进行加强，即这些方向上更快了；对于那些当前的梯度方向与上一次梯度方向不同的参数，那么进行削减，即这些方向上减慢了。因此可以获得更快的收敛速度与减少振荡。



## NAG

Nesterov accelerated gradient
$$
v_t=\gamma v_{t-1}+\eta \triangledown _\theta J(\theta-\gamma v_{t-1})\\\\
\theta =\theta -v_t
$$
向前看了一步。



## Adagrad

是一种基于梯度的优化算法，它能够对每个参数自适应不同的学习速率，对稀疏特征，得到大的学习更新，对非稀疏特征，得到较小的学习更新，因此该优化算法适合处理稀疏特征数据。

对于每一个模型参数$\theta_i$使用不同的学习率$\eta_i$，假设第t次更新中，参数$\theta_i$的梯度为$g_{t,i}$，即：
$$
g_{t,i}=\triangledown _\theta J(\theta_i)
$$
那么有：
$$
\theta_{t+1,i}=\theta_{t,i}-\eta_i\cdot g_{t,i}
$$

$$
\eta_i=\frac{\eta}{\sqrt{G_{t,ii+\epsilon}}}
$$

其中，$G_t\in R^{d*d}$的对角矩阵，每个元素是过去当当前第$i$个参数$\theta_i$的梯度的平方和。$\epsilon$是一个平滑参数，为了使得分母不为0(一般$\epsilon=10e-8$)。Adagrad主要优势在于它能够为每个参数自适应不同的学习速率，而一般的人工都是设定为0.01。同时其缺点在于需要计算参数梯度序列平方和，并且学习速率趋势是不断衰减最终达到一个非常小的值。

## RMSprop

$$
E[g^2]_t=\gamma E[g^2]_{t-1}+(1-\gamma)g^2_t\\\\
\theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{E[g^2]_t+\epsilon}}g_t
$$

Hinton建议$\gamma=0.9,\eta=0.001$。$\gamma$相当于动量参数，也相当于指数衰减。



## Adadelta

  和Adagrad相比。一是使用了窗口w；二是对于参数梯度历史窗口序列(不包括当前)不再使用平方和，而是使用均值代替；三是最终的均值是历史窗口序列均值与当前梯度的时间衰减加权平均

 本质上，使用了一种方式去估计学习率。                
$$
\theta_{t+1}=\theta_t-\eta\cdot g_{t,i}\\\\
\Delta\theta=-\eta\cdot g_{t,i}=-\frac{\eta}{\sqrt{E[g^2]_t+\epsilon}}
$$
另外有：
$$
E[\Delta\theta^2]_t=\gamma E[\Delta\theta^2]_{t-1}+(1-\gamma)\Delta\theta^2_t
$$
使得：
$$
\eta=-\frac{\sqrt{E[\Delta\theta^2]_{t-1}+\epsilon}}{\sqrt{E[g^2]_t+\epsilon}}
$$
注意了$\Delta\theta$ 只能获得上个时间上的。

# Adam

Adaptive Moment Estimation(Adam)

$m_t,v_t$分别用来计算指数衰减的梯度和平方梯度均值。
$$
m_t=\beta_1m_{t-1}+(1-\beta_1)g_t\\\\
v_t=\beta_2v_{t-1}+(1-\beta_2)g_t^2
$$
Adam的作者发现，上面的两个值容易偏向于0，尤其是衰减参数趋向于1时，做校正：
$$
\hat m_t=\frac{m_t}{1-\beta_1^t}\\\\
\hat v_t=\frac{v_t}{1-\beta_2^t}
$$
更新如下：
$$
\theta_{t+1}=\theta_t-\frac{\eta}{\sqrt{\hat v_t}+\epsilon}\cdot \hat m_{t}
$$
一般,$\beta_1=0.9,\beta_2=0.999$

## AdaMax

之前的考虑的是L2 范数的梯度，现在推广到p范数：
$$
v_t=\beta^p_2v_{t-1}+(1-\beta^p_2)|g_t|^p
$$
无穷范数时：
$$
u_t=\beta^\infty_2v_{t-1}+(1-\beta^\infty_2)|g_t|^\infty\\\\
=max(\beta_2\cdot v_{t-1},|g_t|)
$$


## Nadam

Nesterov-accelerated Adaptive Moment

​    多了一个向前看一步。



**Adam 目前是性能最好的。**



# 四 二阶梯度下降-牛顿法

## 阻尼牛顿法

$$
\lambda_k=\math
$$



## 拟牛顿条件



# 五 并行化



# 参考文献

1. Sebastian Ruder ,An overview of gradient descent optimization algorithms 
2. [https://blog.csdn.net/heyongluoyao8/article/details/52478715](https://blog.csdn.net/heyongluoyao8/article/details/52478715)
3. [http://cs231n.github.io/neural-networks-3/](http://cs231n.github.io/neural-networks-3/)