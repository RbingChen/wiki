---
title : "近似算法"
layout : page
date : 2018-11-24 10:31
---

[TOC]



# 一 投影

点到闭凸集的投影：
$$
proj_C(x)=\mathop{\arg\min}_\limits{y}\ \ I_C(y)+\frac{1}{2}||x-y||^2
$$
其中，$I_C$是闭凸集C的示性函数
$$
I_C(x)=\begin{cases}0&x\in C\\\\ \infty&x\notin C\end{cases}
$$
从几何上的解释就是：点到集合上的投影，就是集合上离点最近的点。

把示性函数替换成一般的凸函数f，推广得到一般的投影算子。
$$
proj_C(x)=\mathop{\arg\min}_\limits{y}\ \ f(y)+\frac{1}{2}||x-y||^2
$$
考虑函数上的点到函数的投影，可以发现最小值点的投影是其本身。几何上来解释就是，在函数最小值等高面上的点的投影是它本身。

点$x$到投影点的方向向量$x-proj_f(x)$与等高面垂直，因此，投影操作可以看成是梯度下降的推广。

有：
$$
p=proj_f(x)=\mathop{\arg\min}_\limits{y}\ \ \lambda f(y)+\frac{1}{2}||x-y||^2\\\\
$$
求导可得：
$$
0=p-x+\lambda \partial f(p)\\\\
x=(I+\lambda \partial f)(p)\\\\
p=(I+\lambda \partial f)^{-1}(x)
$$
次梯度的逆有唯一的像！所以投影算子与次梯度关系为
$$
proj_{\lambda f}=(I+\lambda \partial f)^{-1}
$$
另$p=x_{k+1},x=x_k$，推导得到投影操作，$x_{k+1}=proj_f(x_k)$是投影点，
$$
x_{k+1}=x_k-\lambda\partial f(x_{k+1})
$$
如果$\lambda$很小且函数存在常规梯度(正常值)，则近似有：
$$
x_{k+1}\approx x_k-\lambda\partial f(x_{k})
$$
形式上，**投影操作是梯度下降的一种推广**。看做一种前向梯度下降，即下降的梯度不是在当前点计算得到的，而是在下降的目标点计算得到的。如果投影操作计算方便（有简单的解析解），那么用投影操作做优化可以取代梯度下降，并且可以应用到梯度不存在的函数优化。

近似点算法：求函数f的最小值，利用投影算子是压缩算子，且投影算子的不动点是f的最小值点性质可得迭代近似点算法。即在点$x_k$处，求投影点。             
$$
x_{k+1}=proj_f(x_k)
$$
投影算子就相当于梯度下降的推广，近似点算法可以看做梯度下降求最小值的推广。

次梯度：



# 二 近似梯度法

## forward-backward splitting 

如果目标函数存在不可微分部分，可以将目标函数分解为可微分和不可微部分。
$$
\mathop{\arg\min}_\limits{x}\ \  f(x)+g(x)
$$
假设f可微，g是不可微部分，若p是最优解，则有:
$$
\lambda \partial f(p)+\lambda  \partial g(p)=0\\\\
\lambda \partial f(p)-p+p+\lambda  \partial g(p)=0\\\\
(I-\lambda \partial f)(p)=(I+\lambda \partial g)(p)\\\\
p=(I+\lambda \partial g)^{-1}(p-\lambda \partial f(p))
$$
注意次梯度和投影的关系可得：
$$
p=proj_{\lambda g}(p-\lambda \partial f(p))
$$
不动点方程给出了优化迭代步骤，**先按着可微函数梯度下降，然后对不可微函数做投影下降**。

## Backward-backward splitting

都采用投影来做。
$$
p=proj_g(proj_f{p})
$$

## 投影梯度法

无约束优化：梯度下降法和牛顿法(DFP、BFGS等)，对于L2正则较好处理，但L1正则需要特殊处理。

不等式约束凸优化：1、传统的不等式约束优化算法内点法等；2、投影梯度(次梯度)下降（约束优化表示下），直观含义是每步迭代后，迭代结果可能位于约束集合之外，然后取该迭代结果在约束凸集合上的投影作为新的迭代结果。

<img src="/wiki/static/images/project.png" alt="投影图" />



范数：$(w_1^2+w_2^2+...+w_n^2)^{\frac{1}{2}}$，$(w_1^2+w_2^2+...+w_n^2)$，$|w_1+w_2+...+w_n|$

l1范数在0点处处不可导，l2 范数在$w_i$全为0的时候不可导。

# 三 L1范数

$$
\mathop{\min}_\limits{x}\ \ |x|_1
$$

等效问题为
$$
x^{*}=\mathop{\arg\min}_\limits{y}\ \ \ \lambda|y|_1+\frac{1}{2}||x-y||^2\\\\
$$
$x_k$的投影点就是新的值$x_{k+1}$每次的迭代为：
$$
x_{k+1}=x_k-\lambda \partial (|x_{k+1}|)
$$
**投影的过程就是迭代的过程**，$|x|$ 在0点的次梯度为[-1,1]。假设$\lambda>0$

第一种情况：$x_k>\lambda,x_{k+1}=x_k-\lambda$

第二种情况:  $0<x_k<\lambda$

1. 假设 $x_{k+1}>0$，那么$x_{k+1}=x_k-\lambda <0$，矛盾。
2. 假设 $x_{k+1}<0$，那么$x_{k+1}=x_k+\lambda >0$，矛盾。
3. 假设 $x_{k+1}=0$，那么$x_{k+1}=x_k-\lambda *a=0$，存在次梯度$a=\frac{x_k}{\lambda}<1$。

第三情况：略。

得到：
$$
x_{k+1}=\begin{cases}x_k-\lambda&x_k>\lambda\\\\0&-\lambda\le x_k\le\lambda\\\\ x_k+\lambda&x_k<-\lambda\\\   \end{cases}
$$

# 四 ReLU

$$
f(x)=\begin{cases}x&x>0\\\\0&x\le0\end{cases}
$$

子梯度为[0,1]。
$$
x_{k+1}=x_k-\lambda \partial (|x_{k+1}|)
$$
第一种情况：$x_k>\lambda=>x_{k+1}=x_k-\lambda$

第二种情况:  $0<x_k<\lambda=>x_{k+1}=0$

1. 假设 $x_{k+1}>0$，那么$x_{k+1}=x_k-\lambda <0$，矛盾。
2. 假设 $x_{k+1}<0$，那么$x_{k+1}=x_k+\lambda >0$，矛盾。
3. 假设 $x_{k+1}=0$，那么$x_{k+1}=x_k-\lambda *a=0$，存在次梯度$a=\frac{x_k}{\lambda}<1$。

第三种情况：$x_k<0=>x_{k+1}=x_k$。

1. 假设 $x_{k+1}>0$，那么$x_{k+1}=x_k-\lambda <0$，矛盾。
2. 假设 $x_{k+1}<0$，那么$x_{k+1}=x_k+0 >0$，满足。
3. 假设 $x_{k+1}=0$，那么$x_{k+1}=x_k-\lambda *a<0$,矛盾

那么：
$$
x_{k+1}=\begin{cases}x_k-\lambda&x_k>\lambda\\\\0&-\lambda\le x_k\le\lambda\\\\ x_k&x_k<-\lambda\\\   \end{cases}
$$

# 五 问题

## 1.L1范数为什么能稀疏化？L2为什么不能

1. 软阈值。2.考虑$ ||w||_2=\sqrt{(w_1^2+w_2^2+...+w_n^2)}$只有在任意$w_i=0$的时候不可微，但是，这种情况很难出现，而L1范数，只有任一个为0便不可微。

<img src="/wiki/static/images/L1L2.png" alt="范数对比" />

## 2.子梯度

**凸函数的次梯度一定存在，如果函数f在点x处可微，那么g=∇f(x)，为函数在该点的梯度，且唯一；如果不可微，则次梯度不一定唯一。但是对于非凸函数，次梯度则不一定存在，也不一定唯一**

凸函数定义：如果函数$f$可微，那么当且仅当$dom(f)$为凸集，且任意$x,y\in dom(f)$，有
$$
f(y)\ge f(x)+\partial f(x)(y-x)
$$
则函数$f$是凸函数。次梯度是指在函数$f$上的点$x$满足一下条件$g\in R^n$：
$$
f(y)\ge f(x)+g^T (y-x)
$$
对于L1范数的0点来说：

第一种情况：$x>0$，那么$x>=g*x$，得到$g<=1$

第二种情况:$x<0$，那么$-x>=g*x$，得到$g>=-1$

综上，$-1\le g \le 1$。

其实就下方区域。

# 3.为什么对L1范数要进行处理？

假设学习率是 $\lambda =0.3，x_0=0.1$，使用梯度下降$x_1=0.1-0.3=-0.2，x_2=-0.2+0.3=0.1...$，会导致震荡不收敛，因此需要进行处理。



[https://www.cnblogs.com/EE-NovRain/p/3810737.html](https://www.cnblogs.com/EE-NovRain/p/3810737.html)

[https://tracholar.github.io/wiki/machine-learning/optimization/proximal-algorithm.html](https://tracholar.github.io/wiki/machine-learning/optimization/proximal-algorithm.html)

[http://www.hanlongfei.com/convex/2015/10/02/cmu-10725-subgradidient/](http://www.hanlongfei.com/convex/2015/10/02/cmu-10725-subgradidient/)