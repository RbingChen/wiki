---
title : "模型保存和导入"
layout : page
date : 2019-02-22 16:01
---



关注的问题

怎么保存，保存的形式是什么？

怎么导入，导入的网络怎么重建，怎么再训练？

如何使用导入的预先训练的模型进行微调和修改



# 一  模型的保存

```python
 saver=tf.train.Saver()
 # 有如下几个比较重要的参数，var_list: 存储变量的集合，选择性存储。max_to_keep:保存最近几次的模型
 saver.save(sess,filename,global_step=step)
```

```python
save(  # save()的重要参数
    sess,  # A Session to use to save the variables.
    save_path,  # 文件名前缀
    global_step=None,  # 每`global_step`个迭代新建checkpoint filenames保存当前变量。参数值可以是tensor, tensor name或整型数
    write_meta_graph=True  # 是否保存模型数据
)

```

 .meta文件保存了当前图结构

.index文件保存了当前参数名

.data文件保存了当前参数值



# 二 模型的导入



```python
new_saver = tf.train.import_meta_graph()#加载模型
new_saver.restore(sess, filename)#加载变量和数据
# 一般如下使用方法
new_saver = tf.train.import_meta_graph('my_test_model-1000.meta')
new_saver.restore(sess, tf.train.latest_checkpoint('./'))
```

有可能保存多个，使用get_checkponi_state得到最新的模型文件

```python
ckpt = tf.train.get_checkpoint_state(checkpoint_dir)# 得到最新的模型名字   
saver=tf.train.import_meta_graph(os.path.join(checkpoint_dir, ckpt_name+'.meta'))
saver.restore(sess, os.path.join(checkpoint_dir, ckpt_name))
```

导入后的模型，相当于新构建的图。在图的基础在进行相应的操作。

对于输入需要进行如下处理：

```python
graph = tf.get_default_graph()
w1 = graph.get_tensor_by_name("w1:0")# 为什么0 ，不太清楚，反正都是0
w2 = graph.get_tensor_by_name("w2:0")
```



# 三 模型的修改

## 3.1 在对象内部使用saver和load

   这个时候，不要使用get_tensor_by_name来进行相应的tensor的获取。

```python
#coding:utf-8

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'

import numpy as np
import tensorflow as tf
import xlrd
import re


DATA_FILE = 'data/fire_theft.xls'

# Step 1: read in data from the .xls file
book = xlrd.open_workbook(DATA_FILE, encoding_override="utf-8")
sheet = book.sheet_by_index(0)
data = np.asarray([sheet.row_values(i) for i in range(1, sheet.nrows)])
n_samples = sheet.nrows - 1

class SimpleLr():
    def __init__(self):
        self.X = tf.placeholder(tf.float32, name='X')
        self.Y = tf.placeholder(tf.float32, name='Y')


        self.w = tf.Variable(0.0, name='weights')
        self.b = tf.Variable(0.0, name='bias')
        Y_predicted = self.X * self.w + self.b

        self.loss = tf.square(self.Y - Y_predicted, name='loss')
        self.optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(self.loss)
        self.saver=tf.train.Saver()

    def fit(self,sess,data):

        flag,counter=self.load(sess)
        print("******************************",sess.run(self.w),sess.run(self.b))
        for i in range(50): # train the model 100 epochs
            total_loss = 0
            for x, y in data:
                # Session runs train_op and fetch values of loss
                _, l = sess.run([self.optimizer, self.loss], feed_dict={self.X: x, self.Y:y})
                total_loss += l
            self.save(sess,step=i)
            print('Epoch {0}: {1}'.format(i, total_loss/n_samples))
        print("******************************",sess.run(self.w),sess.run(self.b))

    def load(self,sess):
        checkpoint_dir="./model_dir/"
        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
        if ckpt and ckpt.model_checkpoint_path:
            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)
            self.saver.restore(sess, os.path.join(checkpoint_dir, ckpt_name))
            counter = int(next(re.finditer("(\d+)(?!.*\d)",ckpt_name)).group(0))
            print(" [*] Success to read {}".format(ckpt_name))
            return True, counter
        else:
            print(" [*] Failed to find a checkpoint")
            return False, 0


    def save(self,sess,step=0):
        checkpoint_dir="./model_dir/"
        model_name = "simple_model"


        if not os.path.exists(checkpoint_dir):
            os.makedirs(checkpoint_dir)

        self.saver.save(sess,
                        os.path.join(checkpoint_dir, model_name),
                        global_step=step)
model=SimpleLr()
with tf.Session() as sess:
    # Step 7: initialize the necessary variables, in this case, w and b
    sess.run(tf.global_variables_initializer())
    model.fit(sess,data)
```

# 3.2 一般情况下

这个时候需要使用get_tensor_by_name。注意optimizer貌似使用name来进行获取。

```python
#coding:utf-8

import os
os.environ['TF_CPP_MIN_LOG_LEVEL']='2'
import numpy as np
import tensorflow as tf
import xlrd
import re

DATA_FILE = 'data/fire_theft.xls'

# Step 1: read in data from the .xls file
book = xlrd.open_workbook(DATA_FILE, encoding_override="utf-8")
sheet = book.sheet_by_index(0)
data = np.asarray([sheet.row_values(i) for i in range(1, sheet.nrows)])
n_samples = sheet.nrows - 1

def load(sess):
    checkpoint_dir="./model_dir/"
    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)
    print(ckpt.model_checkpoint_path)
    if ckpt and ckpt.model_checkpoint_path:
        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)

        # 一定要定义一个saver 进行 变量导入
        saver=tf.train.import_meta_graph(os.path.join(checkpoint_dir, ckpt_name+'.meta'))
        saver.restore(sess, os.path.join(checkpoint_dir, ckpt_name))
        counter = int(next(re.finditer("(\d+)(?!.*\d)",ckpt_name)).group(0))
        print(" [*] Success to read {}".format(ckpt_name))
        return True, counter
    else:
        print(" [*] Failed to find a checkpoint")
        return False, 0

def save(sess,step=0):
    checkpoint_dir="./model_dir/"
    model_name = "simple_model"
    
    if not os.path.exists(checkpoint_dir):
        os.makedirs(checkpoint_dir)

    tf.train.Saver().save(sess,
                    os.path.join(checkpoint_dir, model_name),
                    global_step=step)

def fit(sess,data):

    flag,counter=load(sess)
    w=sess.graph.get_tensor_by_name("weights:0")
    b=sess.graph.get_tensor_by_name("bias:0")

    loss=sess.graph.get_tensor_by_name("loss:0")
    X=sess.graph.get_tensor_by_name("X:0")
    Y=sess.graph.get_tensor_by_name("Y:0")

    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001,).minimize(loss)
    print("******************************",sess.run(w),sess.run(b))
    for i in range(50): # train the model 100 epochs
        total_loss = 0
        for x, y in data:
            # Session runs train_op and fetch values of loss
            _, l = sess.run([optimizer,loss], feed_dict={X: x, Y:y})
            total_loss += l
        save(sess,step=i)
        print('Epoch {0}: {1}'.format(i, total_loss/n_samples))
    print("******************************",sess.run(w),sess.run(b))

with tf.Session() as sess:
    
    sess.run(tf.global_variables_initializer())
    fit(sess,data)
```

# 四 saved_model模块

模块主要用于[TensorFlow Serving](https://tensorflow.github.io/serving/architecture_overview)



参考：

1.https://blog.csdn.net/tan_handsome/article/details/79303269

2.https://cv-tricks.com/tensorflow-tutorial/save-restore-tensorflow-models-quick-complete-tutorial/

3.https://blog.csdn.net/thriving_fcl/article/details/75213361