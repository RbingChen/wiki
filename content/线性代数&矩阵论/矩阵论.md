---
title : "矩阵论"
layout : page
date : 2019-03-28 14:01
---

[TOC]



# 一 向量

向量的意义？

​        可以看成同时输入多个值。

向量相乘？

​     
$$
u\cdot v=|u||v|cos\theta
$$
如果 $v$是单位向量，则可以看成是$u$在$v$上的投影长度。

投影的计算方式：
$$
 \hat v=\frac{v}{|v|}\\\\
 proj_v{u}=\hat v |u|cos\theta\\\\
 
$$

# 二 矩阵

矩阵都是线性变化。

线性无关：向量间不能互相表示。
$$
a_1v_1+...+a_iv_i =0
$$
如果存在至少一个$a_i$不等于0，使得上式成立，则称向量组$v_1..v_i$是线性相关的。

基：完备



## 矩阵和向量的乘法

<img src="/wiki/static/images/matrix1.png"  alt="矩阵向量乘法"/>

列角度：多组新的向量作为新空间的张成向量。是一种特定的空间变换。矩阵法可以认为是连个变化的叠加。

行角度：向量和向量的内积。

```python
>>> import numpy as np
>>> a= np.array([[1, 2], [3, 4]])
>>> np.linalg.det(a)
-2.0000000000000004
>>> b=np.array(([[5,6],[7,8]]))
>>> np.linalg.det(b)
-2.000000000000005
>>> np.linalg.det(np.matmul(a,b))
4.000000000000008
```

考虑**非方阵**时：如3x2的矩阵，把2x1 的向量映射到三维；2x3维的矩阵把3x1的矩阵映射到二维。因为二维和三维的测度不同，所以行列式会为0。

## 行列式

描述的线性变换的缩放比例。二维则描述的是面积，三维描述的是体积。具有正负号。

那么很容易理解：$det(AB)=det(A)det(B)$，对于可逆矩阵来说$det(AA^{-1})=1$，而如果不可能则，$det(A)=0$。



## 特征向量

`矩阵乘法对应了一个变换，是把任意一个向量变成另一个方向或长度都大多不同的新向量。在这个变换的过程中，原向量主要发生旋转、伸缩的变化。如果矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。`

特征方向为变换的主方向，类比基底，特征值是特征方向上的缩放因子，也表示重要程度。这一点可从特征值的定义看出来。 

`应用`:求特征值和特征向量即为求出该矩阵能使哪些向量（当然是特征向量）只发生拉伸，使其发生拉伸的程度如何（特征值大小）。这样做的意义在于，看清一个矩阵在那些方面能产生最大的效果（power），并根据所产生的每个特征向量（一般研究特征值最大的那几个）进行分类讨论与研究。

为什么PCA中选择topK的特征向量？更能保留更多原始信息，比如，在某个特征向量的特征值很大，这个时候，表示这个方向上的变化很强，如果不保留，那么很难区分变化后向量的不同，即可能重叠。

`特征值可以相同，但是特征向量肯定不同，此外，特征向量组是线性无关`,因此可以用来张成空间，也就是说，原始空间可以用这些向量来张成空间，使用矩阵变化后的空间也可以用这些向量来进行空间张成。？？如下：

原始空间上的向量$g$使用 $e_i$作为基底，同时也可以使用特征向量作为基底。$g=\sum_i^nk_iv_i$，那么$Ag=\sum_i^nk_i{\lambda}_iv_i$，如果使用$e_i$，其在新空间已经不同了？？？

1. 不是所有的方阵都有特征向量。也就是$det(A-\lambda I)=0$无解。

# 相关资料

1.[https://blog.csdn.net/hjq376247328/article/details/80640544](https://blog.csdn.net/hjq376247328/article/details/80640544)

2.3Blue1Brown的视频。

3.[知乎相关的介绍](https://www.zhihu.com/question/20507061?sort=created)