<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>Deep learning theory - Knowledge Change Destiny</title>
        <meta name="keywords" content="Mechine Learning , Economics , Mathematics"/>
        <meta name="description" content="More Reading , More Possibilities"/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)'], ['$', '$']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Google Adsense -->
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');


            // Google Adsense Auto AD
            (adsbygoogle = window.adsbygoogle || []).push({});
            /*
             (adsbygoogle = window.adsbygoogle || []).push({
                  google_ad_client: "ca-pub-6300557868920774",
                  enable_page_level_ads: true
             });
             */
        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#Mechine  Learning">Mechine  Learning</a>&nbsp;»&nbsp;Deep learning theory</div>
</div>
<div class="clearfix"></div>
<div id="title">Deep learning theory</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#deep-vs-shallow">一 deep vs  shallow</a><ul>
<li><a href="#11">1.1定义</a></li>
<li><a href="#12">1.2证明</a></li>
<li><a href="#13">1.3 过拟合</a></li>
</ul>
</li>
<li><a href="#_1">参考论文</a></li>
</ul>
</div>
<h1 id="deep-vs-shallow">一 deep vs  shallow</h1>
<h2 id="11">1.1定义</h2>
<p>Universality定理: Shallow network 可以fit 任何函数。</p>
<p>满足条件<br />
$$<br />
||f(x_1)-f(x_2)||\le L||x_1-x_2||<br />
$$<br />
的函数，被称为L-Lipschitz Function，Lipschitz条件用以保证函数足够smooth，以及函数的连续性，不能确保函数处处可微。</p>
<h2 id="12">1.2证明</h2>
<p>简化处理，在$[0,1]$区间内考虑函数的拟合，给定误差$\epsilon$的情况下，如果函数满足<br />
$$<br />
\int_{0}^{1}|f(x)-f^{\ast}(x)|^2 dx\le\epsilon<br />
$$<br />
则认为存在一个函数能够拟合函数$f(x)$。给出一更严格的定义：<br />
$$<br />
\mathop{\max}_{0\le x\le1}|f(x)-f^\ast(x)|\ \ \le \epsilon<br />
$$<br />
<img src="/wiki/static/images/DeepThoery1.png"  alt="曲线拟合"/></p>
<p>使用线性的片段函数(piecewise linear function)对函数进行拟合。</p>
<p><img src="/wiki/static/images/DeepThoery2.png"  alt="切片"/></p>
<p>使用relu函数进行叠加，能够拟合出不同的片段。只要使用足够的神经元，Shallow network可以拟合任何函数。</p>
<p><img src="/wiki/static/images/DeepThoery3.png"  alt="推论"/></p>
<p><img src="/wiki/static/images/DeepThoery4.png"  alt="推论"/></p>
<p>由[1,2]的推论可知，相同神经元的情况下，deep network将有更多的linear region，且随深度指数增加。</p>
<p>论文[3]，从实验的角度证明了，linear region的数量随着深度而指数增加。</p>
<p><img src="/wiki/static/images/DeepThoery5.png"  alt="推论"/></p>
<p>如上图，左图表示神经元固定时，深度越深，transition(论文的transition描述的是，转折点，如relu的0点，与linear region等效)指数增加。此外，论文从实验角度证明了，网络的不同层对最终结果的影响程度不一样，越是low的层，对结果影响越大。很符合直观上地认识，浅层提取的信息保真度越高，才能保证deep层的有效处理。</p>
<p>架构不同，一般用同样的参数量进行比较不同网络的好坏。</p>
<p>Q1:需要多少神经元去拟合一个 L-Lipschitz function f。</p>
<p>参数更多，表征了更强。<br />
$$<br />
w_1x_1+...+w_nx_n<br />
$$</p>
<p>$$<br />
w_1x_1x_2+w_2x_3x_4<br />
$$</p>
<p>深度网络，出现更多的交叉或者是高阶项。低阶项难以表述高阶。从这里可以思考，deepctr模型，dnn部分用来学习更高阶(更高层交叉，隐式交叉)的特征。</p>
<p>空间上的拟合，是不是有点像是决策树了，决策树是已知数值上进行求解，不能得到未知的数值(值域确定)，而NN不一样，可以通过特征组合，得到不同的值(值域不确定)。</p>
<h2 id="13">1.3 过拟合</h2>
<p>Deep 的拟合能力强，是否存在过拟合问题？</p>
<p>直接使用全连接不就行了，为啥还需要CNN、lstm。</p>
<p>lstm：解决时隙问题，cnn可以取代了。</p>
<p>为什么deep 会比 shallow好？</p>
<ol>
<li>逻辑电路，更加有效。2. 能表达更复杂的关系。3.从参数量来说，其实同样的神经元deep模型的参数量会更多，也就意味着能够拟合更复杂的函数。</li>
</ol>
<p>参数量的区别？</p>
<p>为什么不考虑sigmoid？很难进行理论分析。相当于是很多sigmoid 函数值的叠加。</p>
<p>另一方面，考虑relu的取值范围是[0,1]，是否表征能力会跟强。全连接导致梯度爆炸，所以lstm一般不用relu？</p>
<h1 id="_1">参考论文</h1>
<ol>
<li>Razvan Pasacanu , On the number of response regions of deep feedforward networks with piecewise linear activations,ICLR,2014</li>
<li>G.F Montufar , On the Number of Linear Regions of Deep Neural Networks,NIPS,2014</li>
<li>M. Rahu , On the Expressive Power of Deep Neural Networks ,ICML,2017</li>
</ol>
</div>
<div id="income">
    <img src="/wiki/static/images/galaxy.jpg" alt="星空" style="max-width:300px;" />

    <ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
</div>
<div id="content-footer">created in <span class="create-date date"> 2019-02-24 14:01 </span></div>

<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: 'Deep learning theory',
  owner: 'RbingChen',
  repo: 'wiki',
  oauth: {
    client_id: '8d8c2034c8f3db5fd412',
    client_secret: 'b5dabd27e8c79eb5136fba7730d78f403ea54991',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2019 Cimon.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>