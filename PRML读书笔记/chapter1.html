<!DOCTYPE HTML>
<html>
    <head>
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
        <link rel="Stylesheet" type="text/css" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/normalize.css">
        <link rel="stylesheet" href="/wiki/static/plugin/tipuesearch/css/tipuesearch.css">
        <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
        <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
        <title>chapter1 - Knowledge Change Destiny</title>
        <meta name="keywords" content="Mechine Learning , Economics , Mathematics"/>
        <meta name="description" content="More Reading , More Possibilities"/>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <meta name="viewport" content="width=device-width" />

        <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
          tex2jax: {inlineMath: [['$(',')$'], ['\\(','\\)'], ['$', '$']]}
        });
        </script>
        <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script src="https://code.jquery.com/jquery-2.2.4.min.js"
            integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44="
            crossorigin="anonymous"></script>

        <!-- Google Adsense -->
        <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>

        <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

          ga('create', 'UA-78529611-1', 'auto');
          ga('send', 'pageview');


            // Google Adsense Auto AD
            (adsbygoogle = window.adsbygoogle || []).push({});
            /*
             (adsbygoogle = window.adsbygoogle || []).push({
                  google_ad_client: "ca-pub-6300557868920774",
                  enable_page_level_ads: true
             });
             */
        </script>
    </head>

    <body>
        <div id="container">
            
<div id="header">
  <div id="post-nav"><a href="/wiki/">Home</a>&nbsp;»&nbsp;<a href="/wiki/#PRML读书笔记">PRML读书笔记</a>&nbsp;»&nbsp;chapter1</div>
</div>
<div class="clearfix"></div>
<div id="title">chapter1</div>
<div id="content">
  <div class="toc"><span class="toctitle">Table of Contents</span><ul>
<li><a href="#_1">一 曲线拟合问题</a></li>
<li><a href="#_2">二 概率论</a></li>
<li><a href="#_3">三 贝叶斯</a></li>
<li><a href="#_4">四 概率建模</a></li>
<li><a href="#_5">五 模型选择、维度灾难</a></li>
<li><a href="#_6">参考文献</a></li>
</ul>
</div>
<h1 id="_1">一 曲线拟合问题</h1>
<p>目的： 在现有数据集上，当给定任意新的$x$，给出相应预测值$y$。</p>
<p>优化：erro function，最小化。</p>
<p>模型选择：对比模型选择合适数量的参数个数。</p>
<p><code>over-fitting</code>的概念：在训练数据上表现优异，在待预测数据上表现很差。训练数据越多，能够避免over-fitting。</p>
<p><code>模型复杂度的衡量</code>：参数少的模型所在空间是参数多的模型空间的子空间。参数的数量不是最合适的衡量模型复杂度的指标。第三章，会有介绍？贝叶斯方法依据数据集大小自适应调整参数个数。</p>
<p><code>正则化</code>：限制参数大小。 weight decay。</p>
<p>测试集：留出一部分数据作为测试，但是存在数据浪费，因此需要更精妙的算法来解决。</p>
<h1 id="_2">二 概率论</h1>
<p><code>概率密度函数</code>: 函数值大于0；积分为1。</p>
<p><code>复合函数</code>：$x=g(y)$，$p_y(y)=p_x(x)|\frac{dx}{dy}|$。可以在$(x,x+\delta x)$或者为$(g^{-1}(x),g^{-1}(x)+\delta x)$，转化为$y$的区间：$(y,y+\delta y)$。则$p_x(x)\delta x \approx p_y(y)\delta y$。</p>
<p><code>部分期望</code>：$E_x[f(x,y)]=\sum_x p(x,y)f(x,y)$，换成积分，将得到的是关于y的函数。</p>
<p><code>条件期望</code>：$E_x[f(|y)]=\sum_x p(x|y)f(x)$</p>
<h1 id="_3">三 贝叶斯</h1>
<p>考虑曲线拟合问题，给定数据集$D$，求解最优参数$w$。</p>
<p><code>后验概率</code>：$p(w|D)$，现有数据下，参数的分布概率。</p>
<p><code>先验概率</code>:$p(w)$，是一个概率分布，和数据集无关，人对某个问题的先有认知。</p>
<p><code>似然函数</code>:$p(D|w)$，无论是频率学派还是贝叶斯学派，似然函数都是核心。</p>
<p><strong>贝叶斯 vs 频率学</strong>：</p>
<ol>
<li>主要区别定在于贝叶斯认为参数$w$服从某个概率分布，而频率学派认为$w$是固定值。</li>
<li>频率学派使用最大似然，$max\ \ p(D|w)$。求解参数使得数据集出现的概率最大。一般地，使用最大似然函数的<code>负对数</code>作为erro function。最小误差和最大似然等效。</li>
<li>频率学派怼贝叶斯学派，先验分布的选择不科学，只是觉得选这个先验好计算。第二章就知道了。因此，这也衍生出 <code>noninformative prior</code>。差的先验搞不好就翻车了。注意了，什么交叉验证，都是频率学派的技术，这些技术一定程度上缓解了拟合不好的问题。</li>
</ol>
<p><strong>Why 贝叶斯？</strong></p>
<p>千年不变的例子，抛硬币，很不巧，抛了5次，全是正面，频率学派认为正面概率是1，但是贝叶斯学派不是。</p>
<p>个人觉得都是胡扯，在大数据量的情况下，不可能发生这种事，这决定了贝叶斯学派在工业界上用得不多。不过，在小样本的情况下，贝叶斯学派可能更work，主要体现在先验知识对问题的校正。话说，现在的强化学习，走小样本路线。</p>
<p><strong>贝叶斯有啥用呢</strong></p>
<p>variational Bayes、expectation propagation、一些 采样方法。</p>
<h1 id="_4">四 概率建模</h1>
<p>上一部分提到的似然函数、先验概率、后验概率如何得到。</p>
<p><code>有偏、无偏</code>的问题？ 参数少且数据量大时，有偏影响可以忽略。但是对于参数很多的情况，即使数据量大，这也会是一个严重的问题。<code>有偏是over-fitting根源所在</code>。</p>
<p>引入概率分布：对于任意给定输入$x_n$，真实值$y_n$服从服从均值为$f(x_n,w)$的高斯分布:</p>
<p>$p(y_n|x_n,w,\beta)=N(y_n|f(x_n,w),\beta^{-1})$</p>
<p><strong>为什么这么假设</strong>？</p>
<p>实质上是对误差进行了假设，任务误差服从高斯分布（很常理的假设)。另一种说法是加了<code>噪声带</code>，保证真实值能够被（预测值+噪声）的区间覆盖。</p>
<p><strong>推导问题</strong></p>
<p>1.参数无先验的情况下:<br />
$$<br />
ln\ p(Y|w)=-\frac{\beta}{2}\sum_n{f(x_n,w)-y_n}^2+\frac{N}{2}ln \beta+const<br />
$$<br />
使用最大似然(MLE)求解，得到的最终项和平方误差最小是等效。</p>
<p>2.参数有先验的情况下，$p(w|\alpha)=N(w|0,\alpha ^{-1}I)$，$p(w|Y,X,\alpha,\beta) \propto  p(Y|X,w,\beta)p(w|\alpha)$:<br />
$$<br />
ln\ p(Y|w)=-\frac{\beta}{2}\sum_n{f(x_n,w)-y_n}^2+\frac{N}{2}ln \beta+\frac{\alpha}{2}w^Tw+const<br />
$$<br />
使用最大后验求解(MAP)，多了一个正则项。参数如果服从拉普拉斯分布，推导得到L1正则。</p>
<p><strong>more 贝叶斯</strong></p>
<p><code>点估计</code>和<code>分布估计</code>。MLE 和MAP都是点估计。而贝叶斯估计是分布估计。但是，前者是估计参数，而后者是估计预测值。如下对于待预测的$x$，得到预测值的$y$的分布：<br />
$$<br />
p(y|x,X,Y)=\int p(y|x,w)p(w|X,Y,\alpha,\beta)dw<br />
$$</p>
<p>上等式是一个高斯分布（第三章可知）。</p>
<h1 id="_5">五 模型选择、维度灾难</h1>
<p>交叉验证、留一法，分析了一堆缺点。然而工业界数据还是很多，有足够的数据来做验证集，能够良好评估模型的泛化性能。</p>
<p>其他方法：</p>
<p><code>Akaike information criterion</code>：$logp(D|w)-M$，M是可调节参数个数。</p>
<p>Bayesian information criterion，第四章介绍。</p>
<p>维度灾难[1，2]定义：<strong>给定数据集的情况下</strong>（很多资料都不特定说明这个条件，如果数据集无限，有个毛的维度灾难），过了某个特征维度临界点，如果还持续的增加特征会导致分类器的性能会下降。</p>
<p>在<strong>数据量一定的情况</strong>下，特征维度越大，空间内数据会越<code>稀疏</code>，越容易得到分类平面把样本很好切分开，但是维度过大，模型的泛化性变差，即过拟合。。</p>
<p>使用非线性分类器(NN、决策树、knn等)时，（训练集上）分类效果好，但泛化性差，易过拟合，因此要控制特征维度；朴素贝叶斯、线性分类器，泛化性好，不易过拟合，因此特征维度可以适当更高。——不是很理解。。。</p>
<p><code>overfitting occurs both when estimating relatively few parameters in a highly dimensional space, and when estimating a lot of parameters in a lower dimensional space</code></p>
<p>​    高维空间，只有少数参数也会过拟合，例如：决策树。低纬空间，参数多会过拟合，显然地。而一般的维度灾难描述的是，特征维度越来越高，参数维度越来越高，由于数据量不变而使数据在空间过于稀疏导致模型过拟合。</p>
<p>总结：1.理论上数据无限，则不会发生维度灾难；2. 维度增加，训练样本数需要指数增加。</p>
<h1 id="_6">参考文献</h1>
<p>1.<a href="https://zhuanlan.zhihu.com/p/27488363"><a href="https://zhuanlan.zhihu.com/p/27488363">https://zhuanlan.zhihu.com/p/27488363</a></a></p>
<p>2.<a href="http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/"><a href="http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/">http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/</a></a></p>
</div>
<div id="income">
    <img src="/wiki/static/images/galaxy.jpg" alt="星空" style="max-width:300px;" />

    <ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-6300557868920774"
     data-ad-slot="6882414849"></ins>
</div>
<div id="content-footer">created in <span class="create-date date"> 2019-04-12 10:44 </span></div>

<div id="comments"></div>
<link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css">
<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>
<script type="text/javascript">
const gitment = new Gitment({
  id: location.pathname,
  title: 'chapter1',
  owner: 'RbingChen',
  repo: 'wiki',
  oauth: {
    client_id: '8d8c2034c8f3db5fd412',
    client_secret: 'b5dabd27e8c79eb5136fba7730d78f403ea54991',
  },
  // ...
  // For more available options, check out the documentation below
})

gitment.render('comments')
// or
// gitment.render(document.getElementById('comments'))
// or
// document.body.appendChild(gitment.render())
</script>

        </div>
        <div id="footer">
            <span>
                Copyright © 2019 Cimon.
                Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.
            </span>
        </div>
        

        <script>
        var _hmt = _hmt || [];
        (function() {
          var hm = document.createElement("script");
          hm.src = "https://hm.baidu.com/hm.js?df74779713027375e7b79302fb72d7b0";
          var s = document.getElementsByTagName("script")[0];
          s.parentNode.insertBefore(hm, s);
        })();
        </script>


        <script src="/wiki/tipuesearch_content.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch_set.js"></script>
        <script src="/wiki/static/plugin/tipuesearch/tipuesearch.min.js"></script>
    </body>
</html>